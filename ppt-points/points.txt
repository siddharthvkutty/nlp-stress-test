we know that models fail under certain linguistic stressors, but with a larger targeted corpus, we can prove that the failure patterns are systematic, not anecdotal.


1. Prompt-based mitigation (LLM-only)
Example:
“Be careful with sarcasm and negation.”
This:
Works only for instruction-following models
Does not apply to BERT classifiers
That’s what I meant when I said “LLM-only”.
